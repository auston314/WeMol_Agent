{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77428142-6e49-4182-984b-51f223eb7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3bd899-6f72-4198-9778-7502464b67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from content_tree import *\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f563ab-f9e3-472a-870f-50fd40ae26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    tree = ContentTree()\n",
    "    \n",
    "    # Build the textbook tree from markdown files\n",
    "    md_directory = \"/Users/chemxai/GenAI/AI_Tutor/mcp_kb/md_files\"\n",
    "    tree.build_textbook_tree(md_directory)\n",
    "    \n",
    "    # Print tree structure\n",
    "    print(\"Textbook Structure:\")\n",
    "    tree.print_tree_structure()\n",
    "    \n",
    "    # Get all nodes\n",
    "    all_nodes = tree.tree_node_iterator()\n",
    "    print(f\"\\nTotal nodes: {len(all_nodes)}\")\n",
    "    \n",
    "    # Find a specific chapter\n",
    "    chapter1 = tree.find_node_by_header(\"Chapter 1 - Essential Ideas\")\n",
    "    if chapter1:\n",
    "        print(f\"\\nFound: {chapter1}\")\n",
    "        print(f\"Content preview: {chapter1.content_text[:200]}...\")\n",
    "    \n",
    "    # Get content from a node and its children\n",
    "    if chapter1:\n",
    "        section_content = tree.content_retriever(chapter1)\n",
    "        print(f\"\\nChapter 1 total content length: {len(section_content)} characters\")\n",
    "    \n",
    "    # Test: Print headers of all child nodes to verify order\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ORDER VERIFICATION: All Root Child Nodes\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total root children: {len(tree.root.child_nodes)}\")\n",
    "    print(\"\\nOrder of all child nodes:\")\n",
    "    for i, child in enumerate(tree.root.child_nodes, 1):\n",
    "        print(f\"{i:2d}. [{child.node_id:4d}] Level {child.header_level}: {child.header}\")\n",
    "    \n",
    "    # Verify expected order\n",
    "    expected_order = [\"Preface\"] + [f\"Chapter {i} -\" for i in range(1, 22)] + [f\"Appendix {chr(65+i)}\" for i in range(13)]\n",
    "    print(f\"\\nExpected count: Preface(1) + Chapters(21) + Appendices(13) = 35 total\")\n",
    "    print(f\"Actual count: {len(tree.root.child_nodes)}\")\n",
    "    \n",
    "    # Check if order matches expected pattern\n",
    "    order_correct = True\n",
    "    for i, child in enumerate(tree.root.child_nodes):\n",
    "        if i == 0 and not child.header.startswith(\"Preface\"):\n",
    "            order_correct = False\n",
    "            print(f\"‚ùå Position {i+1}: Expected Preface, got '{child.header}'\")\n",
    "        elif 1 <= i <= 21 and not child.header.startswith(f\"Chapter {i}\"):\n",
    "            order_correct = False\n",
    "            print(f\"‚ùå Position {i+1}: Expected Chapter {i}, got '{child.header}'\")\n",
    "        elif i > 21 and not child.header.startswith(f\"Appendix {chr(65+i-22)}\"):\n",
    "            order_correct = False\n",
    "            print(f\"‚ùå Position {i+1}: Expected Appendix {chr(65+i-22)}, got '{child.header}'\")\n",
    "    \n",
    "    if order_correct:\n",
    "        print(\"‚úÖ Order verification: All nodes are in correct order!\")\n",
    "    else:\n",
    "        print(\"‚ùå Order verification: Some nodes are out of order.\")\n",
    "\n",
    "def test1():\n",
    "    # Create a ContentTree instance\n",
    "    tree = ContentTree()\n",
    "\n",
    "    # Build the textbook tree from markdown files\n",
    "    md_directory = \"/Users/chemxai/GenAI/AI_Tutor/mcp_kb/md_files\"\n",
    "    tree.build_textbook_tree(md_directory)\n",
    "\n",
    "    # Rename repeating headers to make them unique\n",
    "    tree.rename_repeating_headers()\n",
    "\n",
    "    # Print tree structure\n",
    "    print(\"Textbook Structure:\")\n",
    "    tree.print_tree_structure()\n",
    "\n",
    "    # Generate summaries and keywords for all nodes\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING SUMMARIES AND KEYWORDS\")\n",
    "    print(\"=\"*60)\n",
    "    #tree.generate_all_summaries_and_keywords()\n",
    "    tree.process_all_content()\n",
    "\n",
    "    # Print tree structure with summaries and keywords\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TREE STRUCTURE WITH SUMMARIES AND KEYWORDS\")\n",
    "    print(\"=\"*60)\n",
    "    tree.print_tree_structure(show_summary=True, show_keywords=True)\n",
    "\n",
    "    # Get all nodes\n",
    "    all_nodes = tree.tree_node_iterator()\n",
    "    print(f\"\\nTotal nodes: {len(all_nodes)}\")\n",
    "\n",
    "    # Find a specific chapter and show its enhanced information\n",
    "    chapter1 = tree.find_node_by_header(\"Chapter 1 - Essential Ideas\")\n",
    "    if chapter1:\n",
    "        print(f\"\\nFound: {chapter1}\")\n",
    "        print(f\"Content preview: {chapter1.content_text[:200]}...\")\n",
    "        print(f\"Summary: {chapter1.summary}\")\n",
    "        print(f\"Keywords: {', '.join(chapter1.keywords)}\")\n",
    "\n",
    "    # Get content from a node and its children\n",
    "    if chapter1:\n",
    "        section_content = tree.content_retriever(chapter1)\n",
    "        print(f\"\\nChapter 1 total content length: {len(section_content)} characters\")\n",
    "\n",
    "    # Test: Print headers of all child nodes to verify order\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ORDER VERIFICATION: All Root Child Nodes\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total root children: {len(tree.root.child_nodes)}\")\n",
    "    print(\"\\nOrder of all child nodes:\")\n",
    "    for i, child in enumerate(tree.root.child_nodes, 1):\n",
    "        print(f\"{i:2d}. [{child.node_id:4d}] Level {child.header_level}: {child.header}\")\n",
    "\n",
    "    # Verify expected order\n",
    "    expected_order = [\"Preface\"] + [f\"Chapter {i} -\" for i in range(1, 22)] + [f\"Appendix {chr(65+i)}\" for i in range(13)]\n",
    "    print(f\"\\nExpected count: Preface(1) + Chapters(21) + Appendices(13) = 35 total\")\n",
    "    print(f\"Actual count: {len(tree.root.child_nodes)}\")\n",
    "\n",
    "    # Check if order matches expected pattern\n",
    "    order_correct = True\n",
    "    for i, child in enumerate(tree.root.child_nodes):\n",
    "        if i == 0 and not child.header.startswith(\"Preface\"):\n",
    "            order_correct = False\n",
    "            print(f\"‚ùå Position {i+1}: Expected Preface, got '{child.header}'\")\n",
    "        elif 1 <= i <= 21 and not child.header.startswith(f\"Chapter {i}\"):\n",
    "            order_correct = False\n",
    "            print(f\"‚ùå Position {i+1}: Expected Chapter {i}, got '{child.header}'\")\n",
    "        elif i > 21 and not child.header.startswith(f\"Appendix {chr(65+i-22)}\"):\n",
    "            order_correct = False\n",
    "            print(f\"‚ùå Position {i+1}: Expected Appendix {chr(65+i-22)}, got '{child.header}'\")\n",
    "\n",
    "    if order_correct:\n",
    "        print(\"‚úÖ Order verification: All nodes are in correct order!\")\n",
    "    else:\n",
    "        print(\"‚ùå Order verification: Some nodes are out of order.\")\n",
    "\n",
    "    # Show some examples of generated summaries and keywords\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXAMPLES OF GENERATED SUMMARIES AND KEYWORDS\")\n",
    "    print(\"=\"*60)\n",
    "    content_nodes = [node for node in all_nodes if node.header_level > 0 and node.content_text.strip()]\n",
    "    for i, node in enumerate(content_nodes[:5]):  # Show first 5 content nodes\n",
    "        print(f\"\\nNode {i+1}: {node.header}\")\n",
    "        print(f\"Content length: {len(node.content_text)} characters\")\n",
    "        print(f\"Summary: {node.summary}\")\n",
    "        print(f\"Keywords: {', '.join(node.keywords)}\")\n",
    "        print(\"-\" * 40)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13e37e-2618-40c4-b517-623ce91eca6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_tree = test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a66d8b-0f4f-4223-96b2-e583c72b04b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING UNIFIED SEARCH FUNCTIONALITY\n",
      "================================================================================\n",
      "Building tree from: /Users/chemxai/GenAI/AI_Tutor/mcp_kb/md_files\n",
      "\n",
      "Processing content and creating search indexes...\n",
      "================================================================================\n",
      "COMPREHENSIVE CONTENT TREE PROCESSING\n",
      "================================================================================\n",
      "Processing 55 content nodes...\n",
      "LLM Model: qwen2.5vl:32b\n",
      "Embedding Model: text-embedding-3-large\n",
      "Generate Embeddings: False\n",
      "Create Inverse Index: True\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1/55] Processing node 2: Preface\n",
      "Content length: 205 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 106 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[2/55] Processing node 3: About OpenStax\n",
      "Content length: 668 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 151 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[3/55] Processing node 4: About OpenStax resources Customization\n",
      "Content length: 944 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 204 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[4/55] Processing node 5: Errata\n",
      "Content length: 535 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 195 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 6 items\n",
      "\n",
      "[5/55] Processing node 6: Format\n",
      "Content length: 107 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 102 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 1 items\n",
      "\n",
      "[6/55] Processing node 7: About Chemistry 2e\n",
      "Content length: 744 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 191 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 5 items\n",
      "\n",
      "[7/55] Processing node 8: Coverage and scope\n",
      "Content length: 732 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 192 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 5 items\n",
      "\n",
      "[8/55] Processing node 9: Changes to the second edition\n",
      "Content length: 1681 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 171 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 4 items\n",
      "  ‚úì Sentences: 15 items\n",
      "\n",
      "[9/55] Processing node 10: Pedagogical foundation and features\n",
      "Content length: 931 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 152 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 4 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[10/55] Processing node 11: Comprehensive art program\n",
      "Content length: 253 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 115 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 4 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[11/55] Processing node 12: Interactives that engage\n",
      "Content length: 226 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 151 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[12/55] Processing node 13: Assessments that reinforce key concepts\n",
      "Content length: 316 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 170 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[13/55] Processing node 15: Student and instructor resources\n",
      "Content length: 367 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 149 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 3 items\n",
      "\n",
      "[14/55] Processing node 16: Community Hubs\n",
      "Content length: 757 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 159 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[15/55] Processing node 17: Technology partners\n",
      "Content length: 244 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 131 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[16/55] Processing node 19: Paul Flowers, University of North Carolina at Pembroke\n",
      "Content length: 503 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 186 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 3 items\n",
      "\n",
      "[17/55] Processing node 20: Klaus Theopold, University of Delaware\n",
      "Content length: 682 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 192 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 5 items\n",
      "\n",
      "[18/55] Processing node 21: Richard Langley, Stephen F. Austin State University\n",
      "Content length: 649 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 227 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[19/55] Processing node 23: Contributing authors\n",
      "Content length: 583 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 118 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 1 items\n",
      "\n",
      "[20/55] Processing node 24: Reviewers\n",
      "Content length: 2280 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 85 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 1 items\n",
      "\n",
      "[21/55] Processing node 25: Chapter 1 - Essential Ideas\n",
      "Content length: 1971 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 141 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 13 items\n",
      "\n",
      "[22/55] Processing node 26: 1.1 Chemistry in Context\n",
      "Content length: 2726 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 237 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 5 items\n",
      "  ‚úì Sentences: 19 items\n",
      "\n",
      "[23/55] Processing node 27: Chemistry: The Central Science\n",
      "Content length: 2763 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 203 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 17 items\n",
      "\n",
      "[24/55] Processing node 28: The Scientific Method\n",
      "Content length: 1900 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 176 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 14 items\n",
      "\n",
      "[25/55] Processing node 29: The Domains of Chemistry\n",
      "Content length: 3742 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 163 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 7 items\n",
      "  ‚úì Sentences: 27 items\n",
      "\n",
      "[26/55] Processing node 30: 1.2 Phases and Classification of Matter\n",
      "Content length: 5955 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 249 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 16 items\n",
      "  ‚úì Sentences: 44 items\n",
      "\n",
      "[27/55] Processing node 31: Classifying Matter\n",
      "Content length: 6699 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 254 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 20 items\n",
      "  ‚úì Sentences: 47 items\n",
      "\n",
      "[28/55] Processing node 32: Atoms and Molecules\n",
      "Content length: 5066 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 148 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 8 items\n",
      "  ‚úì Sentences: 37 items\n",
      "\n",
      "[29/55] Processing node 33: 1.2 Phases and Classification of Matter Chemistry in Everyday Life\n",
      "Content length: 2214 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 203 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 19 items\n",
      "\n",
      "[30/55] Processing node 34: 1.2 Phases and Classification of Matter Chemistry in Everyday Life\n",
      "Content length: 1428 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 186 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 9 items\n",
      "\n",
      "[31/55] Processing node 35: 1.3 Physical and Chemical Properties\n",
      "Content length: 5010 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 182 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 9 items\n",
      "  ‚úì Sentences: 34 items\n",
      "\n",
      "[32/55] Processing node 36: 1.3 Physical and Chemical Properties Chemistry in Everyday Life\n",
      "Content length: 4235 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 182 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 9 items\n",
      "  ‚úì Sentences: 17 items\n",
      "\n",
      "[33/55] Processing node 37: 1.4 Measurements\n",
      "Content length: 5170 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 214 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 14 items\n",
      "  ‚úì Sentences: 25 items\n",
      "\n",
      "[34/55] Processing node 38: SI Base Units\n",
      "Content length: 393 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 162 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[35/55] Processing node 39: Length\n",
      "Content length: 929 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 148 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[36/55] Processing node 40: Mass\n",
      "Content length: 761 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 189 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 8 items\n",
      "\n",
      "[37/55] Processing node 41: Temperature\n",
      "Content length: 920 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 143 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[38/55] Processing node 42: Time\n",
      "Content length: 314 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 133 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 3 items\n",
      "\n",
      "[39/55] Processing node 43: Derived SI Units\n",
      "Content length: 197 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 113 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[40/55] Processing node 44: Volume\n",
      "Content length: 1389 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 184 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 4 items\n",
      "  ‚úì Sentences: 14 items\n",
      "\n",
      "[41/55] Processing node 45: Density\n",
      "Content length: 2471 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 128 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 7 items\n",
      "  ‚úì Sentences: 11 items\n",
      "\n",
      "[42/55] Processing node 46: Example 1.1\n",
      "Content length: 1776 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 155 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 11 items\n",
      "  ‚úì Sentences: 10 items\n",
      "\n",
      "[43/55] Processing node 47: Example 1.2\n",
      "Content length: 1731 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 183 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 12 items\n",
      "  ‚úì Sentences: 11 items\n",
      "\n",
      "[44/55] Processing node 48: 1.5 Measurement Uncertainty, Accuracy, and Precision\n",
      "Content length: 925 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 185 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 6 items\n",
      "\n",
      "[45/55] Processing node 49: Significant Figures in Measurement\n",
      "Content length: 5523 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 188 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 9 items\n",
      "  ‚úì Sentences: 39 items\n",
      "\n",
      "[46/55] Processing node 50: Significant Figures in Calculations\n",
      "Content length: 1935 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 190 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 8 items\n",
      "\n",
      "[47/55] Processing node 51: Example 1.3\n",
      "Content length: 937 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 168 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 7 items\n",
      "  ‚úì Sentences: 1 items\n",
      "\n",
      "[48/55] Processing node 52: Example 1.4\n",
      "Content length: 756 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 144 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 10 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[49/55] Processing node 53: Example 1.5\n",
      "Content length: 1440 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 144 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 14 items\n",
      "  ‚úì Sentences: 6 items\n",
      "\n",
      "[50/55] Processing node 54: Example 1.6\n",
      "Content length: 694 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 132 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[51/55] Processing node 55: Example 1.7\n",
      "Content length: 1619 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 203 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 15 items\n",
      "  ‚úì Sentences: 10 items\n",
      "\n",
      "[52/55] Processing node 56: Accuracy and Precision\n",
      "Content length: 2263 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 190 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 13 items\n",
      "\n",
      "[53/55] Processing node 57: 1.6 Mathematical Treatment of Measurement Results\n",
      "Content length: 2790 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 152 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 12 items\n",
      "  ‚úì Sentences: 10 items\n",
      "\n",
      "[54/55] Processing node 58: Conversion Factors and Dimensional Analysis\n",
      "Content length: 2153 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 187 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 8 items\n",
      "  ‚úì Sentences: 16 items\n",
      "\n",
      "[55/55] Processing node 59: Example 1.8\n",
      "Content length: 928 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 124 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 9 items\n",
      "  ‚úì Sentences: 5 items\n",
      "\n",
      "================================================================================\n",
      "CONTENT PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Creating inverse index using InverseIndexBuilder...\n",
      "Building n-gram indexes...\n",
      "Index stats: 2506 monograms, 6559 bigrams, 7235 trigrams\n",
      "‚úì N-gram inverse indexes created successfully!\n",
      "    - Monograms: 2506 terms\n",
      "    - Bigrams: 6559 terms\n",
      "    - Trigrams: 7235 terms\n",
      "‚úì Average nodes per term: 1.92\n",
      "‚úì Most common monogram terms:\n",
      "    'chemistry': 20 nodes\n",
      "    'image': 20 nodes\n",
      "    'learning': 18 nodes\n",
      "    'used': 18 nodes\n",
      "    'one': 18 nodes\n",
      "    'example': 17 nodes\n",
      "    'figure': 17 nodes\n",
      "    'may': 16 nodes\n",
      "    'use': 15 nodes\n",
      "    'mass': 15 nodes\n",
      "\n",
      "================================================================================\n",
      "TREE CONTENT PROCESSING FINISHED!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING SEARCH METHODS\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Testing query: 'chemistry atoms'\n",
      "==================================================\n",
      "\n",
      "1. search_content(use_ngrams=True):\n",
      "   Found 3 results:\n",
      "     1. [29] The Domains of Chemistry (score: 0.333)\n",
      "     2. [32] Atoms and Molecules (score: 0.333)\n",
      "     3. [2] Preface (score: 0.167)\n",
      "\n",
      "2. search_content(use_ngrams=False):\n",
      "   Found 3 results:\n",
      "     1. [29] The Domains of Chemistry (score: 2)\n",
      "     2. [32] Atoms and Molecules (score: 2)\n",
      "     3. [2] Preface (score: 1)\n",
      "\n",
      "3. enhanced_search():\n",
      "   Found 3 results:\n",
      "     1. [29] The Domains of Chemistry (score: 0.133)\n",
      "     2. [32] Atoms and Molecules (score: 0.133)\n",
      "     3. [2] Preface (score: 0.067)\n",
      "\n",
      "4. search_content_tree():\n",
      "   Found 3 nodes:\n",
      "     1. [29] The Domains of Chemistry\n",
      "     2. [32] Atoms and Molecules\n",
      "     3. [2] Preface\n",
      "\n",
      "5. search_inverse_index() [DEPRECATED]:\n",
      "Warning: search_inverse_index() is deprecated. Use search_content() instead.\n",
      "   Found 3 results:\n",
      "     1. [29] The Domains of Chemistry (score: 2)\n",
      "     2. [32] Atoms and Molecules (score: 2)\n",
      "     3. [2] Preface (score: 1)\n",
      "\n",
      "==================================================\n",
      "Testing query: 'measurements accuracy'\n",
      "==================================================\n",
      "\n",
      "1. search_content(use_ngrams=True):\n",
      "   Found 3 results:\n",
      "     1. [48] 1.5 Measurement Uncertainty, Accuracy, and Precision (score: 0.333)\n",
      "     2. [56] Accuracy and Precision (score: 0.333)\n",
      "     3. [25] Chapter 1 - Essential Ideas (score: 0.333)\n",
      "\n",
      "2. search_content(use_ngrams=False):\n",
      "   Found 3 results:\n",
      "     1. [25] Chapter 1 - Essential Ideas (score: 2)\n",
      "     2. [48] 1.5 Measurement Uncertainty, Accuracy, and Precision (score: 2)\n",
      "     3. [56] Accuracy and Precision (score: 2)\n",
      "\n",
      "3. enhanced_search():\n",
      "   Found 3 results:\n",
      "     1. [48] 1.5 Measurement Uncertainty, Accuracy, and Precision (score: 0.133)\n",
      "     2. [56] Accuracy and Precision (score: 0.133)\n",
      "     3. [25] Chapter 1 - Essential Ideas (score: 0.133)\n",
      "\n",
      "4. search_content_tree():\n",
      "   Found 3 nodes:\n",
      "     1. [48] 1.5 Measurement Uncertainty, Accuracy, and Precision\n",
      "     2. [56] Accuracy and Precision\n",
      "     3. [25] Chapter 1 - Essential Ideas\n",
      "\n",
      "5. search_inverse_index() [DEPRECATED]:\n",
      "Warning: search_inverse_index() is deprecated. Use search_content() instead.\n",
      "   Found 3 results:\n",
      "     1. [25] Chapter 1 - Essential Ideas (score: 2)\n",
      "     2. [48] 1.5 Measurement Uncertainty, Accuracy, and Precision (score: 2)\n",
      "     3. [56] Accuracy and Precision (score: 2)\n",
      "\n",
      "==================================================\n",
      "Testing query: 'density volume'\n",
      "==================================================\n",
      "\n",
      "1. search_content(use_ngrams=True):\n",
      "   Found 3 results:\n",
      "     1. [35] 1.3 Physical and Chemical Properties (score: 0.333)\n",
      "     2. [37] 1.4 Measurements (score: 0.333)\n",
      "     3. [43] Derived SI Units (score: 0.333)\n",
      "\n",
      "2. search_content(use_ngrams=False):\n",
      "   Found 3 results:\n",
      "     1. [35] 1.3 Physical and Chemical Properties (score: 2)\n",
      "     2. [37] 1.4 Measurements (score: 2)\n",
      "     3. [43] Derived SI Units (score: 2)\n",
      "\n",
      "3. enhanced_search():\n",
      "   Found 3 results:\n",
      "     1. [35] 1.3 Physical and Chemical Properties (score: 0.133)\n",
      "     2. [37] 1.4 Measurements (score: 0.133)\n",
      "     3. [43] Derived SI Units (score: 0.133)\n",
      "\n",
      "4. search_content_tree():\n",
      "   Found 3 nodes:\n",
      "     1. [35] 1.3 Physical and Chemical Properties\n",
      "     2. [37] 1.4 Measurements\n",
      "     3. [43] Derived SI Units\n",
      "\n",
      "5. search_inverse_index() [DEPRECATED]:\n",
      "Warning: search_inverse_index() is deprecated. Use search_content() instead.\n",
      "   Found 3 results:\n",
      "     1. [35] 1.3 Physical and Chemical Properties (score: 2)\n",
      "     2. [37] 1.4 Measurements (score: 2)\n",
      "     3. [43] Derived SI Units (score: 2)\n",
      "\n",
      "==================================================\n",
      "Testing query: 'scientific method'\n",
      "==================================================\n",
      "\n",
      "1. search_content(use_ngrams=True):\n",
      "   Found 3 results:\n",
      "     1. [26] 1.1 Chemistry in Context (score: 1.000)\n",
      "     2. [28] The Scientific Method (score: 1.000)\n",
      "     3. [37] 1.4 Measurements (score: 0.167)\n",
      "\n",
      "2. search_content(use_ngrams=False):\n",
      "   Found 3 results:\n",
      "     1. [26] 1.1 Chemistry in Context (score: 2)\n",
      "     2. [28] The Scientific Method (score: 2)\n",
      "     3. [9] Changes to the second edition (score: 1)\n",
      "\n",
      "3. enhanced_search():\n",
      "   Found 3 results:\n",
      "     1. [26] 1.1 Chemistry in Context (score: 0.400)\n",
      "     2. [28] The Scientific Method (score: 0.400)\n",
      "     3. [37] 1.4 Measurements (score: 0.067)\n",
      "\n",
      "4. search_content_tree():\n",
      "   Found 3 nodes:\n",
      "     1. [26] 1.1 Chemistry in Context\n",
      "     2. [28] The Scientific Method\n",
      "     3. [37] 1.4 Measurements\n",
      "\n",
      "5. search_inverse_index() [DEPRECATED]:\n",
      "Warning: search_inverse_index() is deprecated. Use search_content() instead.\n",
      "   Found 3 results:\n",
      "     1. [26] 1.1 Chemistry in Context (score: 2)\n",
      "     2. [28] The Scientific Method (score: 2)\n",
      "     3. [9] Changes to the second edition (score: 1)\n",
      "\n",
      "================================================================================\n",
      "TESTING SEARCH INDEX AVAILABILITY\n",
      "================================================================================\n",
      "Has inverse_index_builder: True\n",
      "Has basic inverse_index: True\n",
      "N-gram indexes available:\n",
      "  - Monograms: 2506 terms\n",
      "  - Bigrams: 6559 terms\n",
      "  - Trigrams: 7235 terms\n",
      "\n",
      "================================================================================\n",
      "‚úÖ UNIFIED SEARCH TESTING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üéâ All search methods tested successfully!\n"
     ]
    }
   ],
   "source": [
    "from content_tree import ContentTree\n",
    "\n",
    "def test_unified_search():\n",
    "    \"\"\"Test the unified search methods.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"TESTING UNIFIED SEARCH FUNCTIONALITY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a ContentTree instance\n",
    "    tree = ContentTree()\n",
    "    \n",
    "    # Build the textbook tree from markdown files (limited to 2 files for testing)\n",
    "    md_directory = \"/Users/chemxai/GenAI/AI_Tutor/mcp_kb/md_files\"\n",
    "    print(f\"Building tree from: {md_directory}\")\n",
    "    tree.build_textbook_tree(md_directory)\n",
    "    \n",
    "    # Rename repeating headers to make them unique\n",
    "    tree.rename_repeating_headers()\n",
    "    \n",
    "    # Process content to create search indexes\n",
    "    print(\"\\nProcessing content and creating search indexes...\")\n",
    "    tree.process_tree_content(\n",
    "        max_summary_words=20,\n",
    "        max_keywords=5,\n",
    "        generate_embeddings=False,  # Disabled for faster testing\n",
    "        create_inverse_index=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING SEARCH METHODS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_queries = [\n",
    "        \"chemistry atoms\",\n",
    "        \"measurements accuracy\",\n",
    "        \"density volume\",\n",
    "        \"scientific method\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing query: '{query}'\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Test main search_content method with n-grams\n",
    "        print(\"\\n1. search_content(use_ngrams=True):\")\n",
    "        try:\n",
    "            results_ngrams = tree.search_content(query, max_results=3, use_ngrams=True)\n",
    "            print(f\"   Found {len(results_ngrams)} results:\")\n",
    "            for i, (node_id, score) in enumerate(results_ngrams, 1):\n",
    "                node = next((n for n in tree.tree_node_iterator() if n.node_id == node_id), None)\n",
    "                if node:\n",
    "                    print(f\"     {i}. [{node_id}] {node.header} (score: {score:.3f})\")\n",
    "                else:\n",
    "                    print(f\"     {i}. [Node {node_id} not found] (score: {score:.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "        \n",
    "        # Test main search_content method without n-grams\n",
    "        print(\"\\n2. search_content(use_ngrams=False):\")\n",
    "        try:\n",
    "            results_simple = tree.search_content(query, max_results=3, use_ngrams=False)\n",
    "            print(f\"   Found {len(results_simple)} results:\")\n",
    "            for i, (node_id, score) in enumerate(results_simple, 1):\n",
    "                node = next((n for n in tree.tree_node_iterator() if n.node_id == node_id), None)\n",
    "                if node:\n",
    "                    print(f\"     {i}. [{node_id}] {node.header} (score: {score})\")\n",
    "                else:\n",
    "                    print(f\"     {i}. [Node {node_id} not found] (score: {score})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "        \n",
    "        # Test enhanced search\n",
    "        print(\"\\n3. enhanced_search():\")\n",
    "        try:\n",
    "            enhanced_results = tree.enhanced_search(query, max_results=3)\n",
    "            print(f\"   Found {len(enhanced_results)} results:\")\n",
    "            for i, (node_id, score) in enumerate(enhanced_results, 1):\n",
    "                node = next((n for n in tree.tree_node_iterator() if n.node_id == node_id), None)\n",
    "                if node:\n",
    "                    print(f\"     {i}. [{node_id}] {node.header} (score: {score:.3f})\")\n",
    "                else:\n",
    "                    print(f\"     {i}. [Node {node_id} not found] (score: {score:.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "        \n",
    "        # Test search_content_tree (returns actual nodes)\n",
    "        print(\"\\n4. search_content_tree():\")\n",
    "        try:\n",
    "            node_results = tree.search_content_tree(query, max_results=3, use_ngrams=True)\n",
    "            print(f\"   Found {len(node_results)} nodes:\")\n",
    "            for i, node in enumerate(node_results, 1):\n",
    "                print(f\"     {i}. [{node.node_id}] {node.header}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "        \n",
    "        # Test deprecated method (should show warning)\n",
    "        print(\"\\n5. search_inverse_index() [DEPRECATED]:\")\n",
    "        try:\n",
    "            deprecated_results = tree.search_inverse_index(query, max_results=3)\n",
    "            print(f\"   Found {len(deprecated_results)} results:\")\n",
    "            for i, (node_id, score) in enumerate(deprecated_results, 1):\n",
    "                node = next((n for n in tree.tree_node_iterator() if n.node_id == node_id), None)\n",
    "                if node:\n",
    "                    print(f\"     {i}. [{node_id}] {node.header} (score: {score})\")\n",
    "                else:\n",
    "                    print(f\"     {i}. [Node {node_id} not found] (score: {score})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING SEARCH INDEX AVAILABILITY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test search indexes\n",
    "    print(f\"Has inverse_index_builder: {hasattr(tree, 'inverse_index_builder') and tree.inverse_index_builder is not None}\")\n",
    "    print(f\"Has basic inverse_index: {hasattr(tree, 'inverse_index') and len(tree.inverse_index) > 0}\")\n",
    "    \n",
    "    if hasattr(tree, 'inverse_index_builder') and tree.inverse_index_builder:\n",
    "        print(f\"N-gram indexes available:\")\n",
    "        print(f\"  - Monograms: {len(tree.inverse_index_builder.monogram_index)} terms\")\n",
    "        print(f\"  - Bigrams: {len(tree.inverse_index_builder.bigram_index)} terms\")\n",
    "        print(f\"  - Trigrams: {len(tree.inverse_index_builder.trigram_index)} terms\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ UNIFIED SEARCH TESTING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = test_unified_search()\n",
    "    if success:\n",
    "        print(\"\\nüéâ All search methods tested successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Some tests failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0633c62-1449-434d-9f01-912f0e2cc477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING NORMALIZED N-GRAM SCORING\n",
      "================================================================================\n",
      "Building tree from: /Users/chemxai/GenAI/AI_Tutor/mcp_kb/md_files\n",
      "\n",
      "Processing content and creating search indexes...\n",
      "================================================================================\n",
      "COMPREHENSIVE CONTENT TREE PROCESSING\n",
      "================================================================================\n",
      "Processing 55 content nodes...\n",
      "LLM Model: qwen2.5vl:32b\n",
      "Embedding Model: text-embedding-3-large\n",
      "Generate Embeddings: False\n",
      "Create Inverse Index: True\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1/55] Processing node 2: Preface\n",
      "Content length: 205 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 106 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[2/55] Processing node 3: About OpenStax\n",
      "Content length: 668 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 151 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[3/55] Processing node 4: About OpenStax resources Customization\n",
      "Content length: 944 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 204 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[4/55] Processing node 5: Errata\n",
      "Content length: 535 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 195 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 6 items\n",
      "\n",
      "[5/55] Processing node 6: Format\n",
      "Content length: 107 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 102 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 1 items\n",
      "\n",
      "[6/55] Processing node 7: About Chemistry 2e\n",
      "Content length: 744 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 191 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 5 items\n",
      "\n",
      "[7/55] Processing node 8: Coverage and scope\n",
      "Content length: 732 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 192 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 5 items\n",
      "\n",
      "[8/55] Processing node 9: Changes to the second edition\n",
      "Content length: 1681 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 171 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 4 items\n",
      "  ‚úì Sentences: 15 items\n",
      "\n",
      "[9/55] Processing node 10: Pedagogical foundation and features\n",
      "Content length: 931 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 152 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 4 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[10/55] Processing node 11: Comprehensive art program\n",
      "Content length: 253 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 115 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 4 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[11/55] Processing node 12: Interactives that engage\n",
      "Content length: 226 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 151 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[12/55] Processing node 13: Assessments that reinforce key concepts\n",
      "Content length: 316 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 170 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[13/55] Processing node 15: Student and instructor resources\n",
      "Content length: 367 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 149 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 3 items\n",
      "\n",
      "[14/55] Processing node 16: Community Hubs\n",
      "Content length: 757 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 159 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[15/55] Processing node 17: Technology partners\n",
      "Content length: 244 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 131 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[16/55] Processing node 19: Paul Flowers, University of North Carolina at Pembroke\n",
      "Content length: 503 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 186 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 3 items\n",
      "\n",
      "[17/55] Processing node 20: Klaus Theopold, University of Delaware\n",
      "Content length: 682 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 192 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 5 items\n",
      "\n",
      "[18/55] Processing node 21: Richard Langley, Stephen F. Austin State University\n",
      "Content length: 649 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 227 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[19/55] Processing node 23: Contributing authors\n",
      "Content length: 583 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 118 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 1 items\n",
      "\n",
      "[20/55] Processing node 24: Reviewers\n",
      "Content length: 2280 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 85 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 1 items\n",
      "\n",
      "[21/55] Processing node 25: Chapter 1 - Essential Ideas\n",
      "Content length: 1971 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 141 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 13 items\n",
      "\n",
      "[22/55] Processing node 26: 1.1 Chemistry in Context\n",
      "Content length: 2726 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 237 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 5 items\n",
      "  ‚úì Sentences: 19 items\n",
      "\n",
      "[23/55] Processing node 27: Chemistry: The Central Science\n",
      "Content length: 2763 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 203 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 17 items\n",
      "\n",
      "[24/55] Processing node 28: The Scientific Method\n",
      "Content length: 1900 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 176 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 14 items\n",
      "\n",
      "[25/55] Processing node 29: The Domains of Chemistry\n",
      "Content length: 3742 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 163 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 7 items\n",
      "  ‚úì Sentences: 27 items\n",
      "\n",
      "[26/55] Processing node 30: 1.2 Phases and Classification of Matter\n",
      "Content length: 5955 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 249 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 16 items\n",
      "  ‚úì Sentences: 44 items\n",
      "\n",
      "[27/55] Processing node 31: Classifying Matter\n",
      "Content length: 6699 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 254 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 20 items\n",
      "  ‚úì Sentences: 47 items\n",
      "\n",
      "[28/55] Processing node 32: Atoms and Molecules\n",
      "Content length: 5066 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 148 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 8 items\n",
      "  ‚úì Sentences: 37 items\n",
      "\n",
      "[29/55] Processing node 33: 1.2 Phases and Classification of Matter Chemistry in Everyday Life\n",
      "Content length: 2214 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 203 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 19 items\n",
      "\n",
      "[30/55] Processing node 34: 1.2 Phases and Classification of Matter Chemistry in Everyday Life\n",
      "Content length: 1428 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 186 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 9 items\n",
      "\n",
      "[31/55] Processing node 35: 1.3 Physical and Chemical Properties\n",
      "Content length: 5010 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 182 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 9 items\n",
      "  ‚úì Sentences: 34 items\n",
      "\n",
      "[32/55] Processing node 36: 1.3 Physical and Chemical Properties Chemistry in Everyday Life\n",
      "Content length: 4235 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 182 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 9 items\n",
      "  ‚úì Sentences: 17 items\n",
      "\n",
      "[33/55] Processing node 37: 1.4 Measurements\n",
      "Content length: 5170 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 214 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 14 items\n",
      "  ‚úì Sentences: 25 items\n",
      "\n",
      "[34/55] Processing node 38: SI Base Units\n",
      "Content length: 393 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 162 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[35/55] Processing node 39: Length\n",
      "Content length: 929 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 148 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[36/55] Processing node 40: Mass\n",
      "Content length: 761 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 189 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 8 items\n",
      "\n",
      "[37/55] Processing node 41: Temperature\n",
      "Content length: 920 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 143 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 2 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[38/55] Processing node 42: Time\n",
      "Content length: 314 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 133 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 3 items\n",
      "\n",
      "[39/55] Processing node 43: Derived SI Units\n",
      "Content length: 197 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 113 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 1 items\n",
      "  ‚úì Sentences: 2 items\n",
      "\n",
      "[40/55] Processing node 44: Volume\n",
      "Content length: 1389 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 184 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 4 items\n",
      "  ‚úì Sentences: 14 items\n",
      "\n",
      "[41/55] Processing node 45: Density\n",
      "Content length: 2471 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 128 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 7 items\n",
      "  ‚úì Sentences: 11 items\n",
      "\n",
      "[42/55] Processing node 46: Example 1.1\n",
      "Content length: 1776 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 155 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 11 items\n",
      "  ‚úì Sentences: 10 items\n",
      "\n",
      "[43/55] Processing node 47: Example 1.2\n",
      "Content length: 1731 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 183 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 12 items\n",
      "  ‚úì Sentences: 11 items\n",
      "\n",
      "[44/55] Processing node 48: 1.5 Measurement Uncertainty, Accuracy, and Precision\n",
      "Content length: 925 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 185 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 3 items\n",
      "  ‚úì Sentences: 6 items\n",
      "\n",
      "[45/55] Processing node 49: Significant Figures in Measurement\n",
      "Content length: 5523 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 188 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 9 items\n",
      "  ‚úì Sentences: 39 items\n",
      "\n",
      "[46/55] Processing node 50: Significant Figures in Calculations\n",
      "Content length: 1935 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 190 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 8 items\n",
      "\n",
      "[47/55] Processing node 51: Example 1.3\n",
      "Content length: 937 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 168 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 7 items\n",
      "  ‚úì Sentences: 1 items\n",
      "\n",
      "[48/55] Processing node 52: Example 1.4\n",
      "Content length: 756 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 144 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 10 items\n",
      "  ‚úì Sentences: 7 items\n",
      "\n",
      "[49/55] Processing node 53: Example 1.5\n",
      "Content length: 1440 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 144 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 14 items\n",
      "  ‚úì Sentences: 6 items\n",
      "\n",
      "[50/55] Processing node 54: Example 1.6\n",
      "Content length: 694 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 132 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 4 items\n",
      "\n",
      "[51/55] Processing node 55: Example 1.7\n",
      "Content length: 1619 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 203 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 15 items\n",
      "  ‚úì Sentences: 10 items\n",
      "\n",
      "[52/55] Processing node 56: Accuracy and Precision\n",
      "Content length: 2263 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 190 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 6 items\n",
      "  ‚úì Sentences: 13 items\n",
      "\n",
      "[53/55] Processing node 57: 1.6 Mathematical Treatment of Measurement Results\n",
      "Content length: 2790 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 152 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 12 items\n",
      "  ‚úì Sentences: 10 items\n",
      "\n",
      "[54/55] Processing node 58: Conversion Factors and Dimensional Analysis\n",
      "Content length: 2153 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 187 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 8 items\n",
      "  ‚úì Sentences: 16 items\n",
      "\n",
      "[55/55] Processing node 59: Example 1.8\n",
      "Content length: 928 characters\n",
      "Process node content ........\n",
      "Generating summary...\n",
      "Generating keywords...\n",
      "  ‚úì Summary: 124 chars\n",
      "  ‚úì Keywords: 4 items\n",
      "  ‚úì Chunks: 9 items\n",
      "  ‚úì Sentences: 5 items\n",
      "\n",
      "================================================================================\n",
      "CONTENT PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Creating inverse index using InverseIndexBuilder...\n",
      "Building n-gram indexes...\n",
      "Index stats: 2506 monograms, 6559 bigrams, 7235 trigrams\n",
      "‚úì N-gram inverse indexes created successfully!\n",
      "    - Monograms: 2506 terms\n",
      "    - Bigrams: 6559 terms\n",
      "    - Trigrams: 7235 terms\n",
      "‚úì Average nodes per term: 1.92\n",
      "‚úì Most common monogram terms:\n",
      "    'chemistry': 20 nodes\n",
      "    'image': 20 nodes\n",
      "    'learning': 18 nodes\n",
      "    'used': 18 nodes\n",
      "    'one': 18 nodes\n",
      "    'example': 17 nodes\n",
      "    'figure': 17 nodes\n",
      "    'may': 16 nodes\n",
      "    'use': 15 nodes\n",
      "    'mass': 15 nodes\n",
      "\n",
      "================================================================================\n",
      "TREE CONTENT PROCESSING FINISHED!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING SCORE NORMALIZATION\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Testing query: 'chemistry'\n",
      "Query length: 1 words\n",
      "============================================================\n",
      "\n",
      "RAW SCORES (unbounded):\n",
      "  Max raw score: 1.000\n",
      "    1. [2] Preface (score: 1.000)\n",
      "    2. [4] About OpenStax resources Customization (score: 1.000)\n",
      "    3. [7] About Chemistry 2e (score: 1.000)\n",
      "\n",
      "NORMALIZED SCORES (0-1.0 range):\n",
      "  Max normalized score: 1.000\n",
      "    1. [2] Preface (score: 1.000)\n",
      "    2. [4] About OpenStax resources Customization (score: 1.000)\n",
      "    3. [7] About Chemistry 2e (score: 1.000)\n",
      "\n",
      "Max possible score for this query: 1.000\n",
      "Normalization factor: 1.000\n",
      "\n",
      "============================================================\n",
      "Testing query: 'chemistry atoms'\n",
      "Query length: 2 words\n",
      "============================================================\n",
      "\n",
      "RAW SCORES (unbounded):\n",
      "  Max raw score: 0.667\n",
      "    1. [29] The Domains of Chemistry (score: 0.667)\n",
      "    2. [32] Atoms and Molecules (score: 0.667)\n",
      "    3. [2] Preface (score: 0.333)\n",
      "\n",
      "NORMALIZED SCORES (0-1.0 range):\n",
      "  Max normalized score: 0.333\n",
      "    1. [29] The Domains of Chemistry (score: 0.333)\n",
      "    2. [32] Atoms and Molecules (score: 0.333)\n",
      "    3. [2] Preface (score: 0.167)\n",
      "\n",
      "Max possible score for this query: 2.000\n",
      "Normalization factor: 0.333\n",
      "\n",
      "============================================================\n",
      "Testing query: 'chemistry atoms molecules structure bonds'\n",
      "Query length: 5 words\n",
      "============================================================\n",
      "\n",
      "RAW SCORES (unbounded):\n",
      "  Max raw score: 0.667\n",
      "    1. [32] Atoms and Molecules (score: 0.667)\n",
      "    2. [30] 1.2 Phases and Classification of Matter (score: 0.500)\n",
      "    3. [29] The Domains of Chemistry (score: 0.333)\n",
      "\n",
      "NORMALIZED SCORES (0-1.0 range):\n",
      "  Max normalized score: 0.167\n",
      "    1. [32] Atoms and Molecules (score: 0.167)\n",
      "    2. [30] 1.2 Phases and Classification of Matter (score: 0.125)\n",
      "    3. [29] The Domains of Chemistry (score: 0.083)\n",
      "\n",
      "Max possible score for this query: 4.000\n",
      "Normalization factor: 0.167\n",
      "\n",
      "============================================================\n",
      "Testing query: 'measurements accuracy precision uncertainty'\n",
      "Query length: 4 words\n",
      "============================================================\n",
      "\n",
      "RAW SCORES (unbounded):\n",
      "  Max raw score: 0.889\n",
      "    1. [48] 1.5 Measurement Uncertainty, Accuracy, and Precision (score: 0.889)\n",
      "    2. [25] Chapter 1 - Essential Ideas (score: 0.889)\n",
      "    3. [56] Accuracy and Precision (score: 0.778)\n",
      "\n",
      "NORMALIZED SCORES (0-1.0 range):\n",
      "  Max normalized score: 0.235\n",
      "    1. [48] 1.5 Measurement Uncertainty, Accuracy, and Precision (score: 0.235)\n",
      "    2. [25] Chapter 1 - Essential Ideas (score: 0.235)\n",
      "    3. [56] Accuracy and Precision (score: 0.206)\n",
      "\n",
      "Max possible score for this query: 3.778\n",
      "Normalization factor: 0.235\n",
      "\n",
      "============================================================\n",
      "Testing query: 'scientific method observation hypothesis'\n",
      "Query length: 4 words\n",
      "============================================================\n",
      "\n",
      "RAW SCORES (unbounded):\n",
      "  Max raw score: 0.889\n",
      "    1. [28] The Scientific Method (score: 0.889)\n",
      "    2. [26] 1.1 Chemistry in Context (score: 0.667)\n",
      "    3. [32] Atoms and Molecules (score: 0.111)\n",
      "\n",
      "NORMALIZED SCORES (0-1.0 range):\n",
      "  Max normalized score: 0.235\n",
      "    1. [28] The Scientific Method (score: 0.235)\n",
      "    2. [26] 1.1 Chemistry in Context (score: 0.176)\n",
      "    3. [32] Atoms and Molecules (score: 0.029)\n",
      "\n",
      "Max possible score for this query: 3.778\n",
      "Normalization factor: 0.235\n",
      "\n",
      "================================================================================\n",
      "TESTING ENHANCED SEARCH WITH NORMALIZED SCORES\n",
      "================================================================================\n",
      "\n",
      "Testing enhanced_search with query: 'chemistry atoms molecules'\n",
      "\n",
      "Enhanced search results (using normalized lexical scores):\n",
      "  1. [32] Atoms and Molecules (combined score: 0.140)\n",
      "  2. [30] 1.2 Phases and Classification of Matter (combined score: 0.120)\n",
      "  3. [29] The Domains of Chemistry (combined score: 0.060)\n",
      "  4. [33] 1.2 Phases and Classification of Matter Chemistry in Everyday Life (combined score: 0.040)\n",
      "  5. [2] Preface (combined score: 0.020)\n",
      "\n",
      "================================================================================\n",
      "TESTING SEMANTIC + LEXICAL SCORE BALANCE\n",
      "================================================================================\n",
      "With normalized scores, semantic (0-1.0) and lexical (0-1.0) are now balanced!\n",
      "Example weights: semantic_weight=0.6, lexical_weight=0.4\n",
      "Max combined score would be: 0.6 * 1.0 + 0.4 * 1.0 = 1.0\n",
      "\n",
      "Example score combinations:\n",
      "  High semantic (0.9), High lexical (0.8): 0.860\n",
      "  Medium semantic (0.5), High lexical (0.9): 0.660\n",
      "  High semantic (0.8), Medium lexical (0.4): 0.640\n",
      "\n",
      "================================================================================\n",
      "‚úÖ NORMALIZED SCORING TEST COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from content_tree import ContentTree\n",
    "\n",
    "def test_normalized_scoring():\n",
    "    \"\"\"Test the normalized scoring functionality.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"TESTING NORMALIZED N-GRAM SCORING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a ContentTree instance\n",
    "    tree = ContentTree()\n",
    "    \n",
    "    # Build the textbook tree from markdown files (limited to 2 files for testing)\n",
    "    md_directory = \"/Users/chemxai/GenAI/AI_Tutor/mcp_kb/md_files\"\n",
    "    print(f\"Building tree from: {md_directory}\")\n",
    "    tree.build_textbook_tree(md_directory)\n",
    "    \n",
    "    # Rename repeating headers to make them unique\n",
    "    tree.rename_repeating_headers()\n",
    "    \n",
    "    # Process content to create search indexes\n",
    "    print(\"\\nProcessing content and creating search indexes...\")\n",
    "    tree.process_tree_content(\n",
    "        max_summary_words=20,\n",
    "        max_keywords=5,\n",
    "        generate_embeddings=False,  # Disabled for faster testing\n",
    "        create_inverse_index=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING SCORE NORMALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_queries = [\n",
    "        \"chemistry\",\n",
    "        \"chemistry atoms\",\n",
    "        \"chemistry atoms molecules structure bonds\",\n",
    "        \"measurements accuracy precision uncertainty\",\n",
    "        \"scientific method observation hypothesis\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing query: '{query}'\")\n",
    "        print(f\"Query length: {len(query.split())} words\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Test with InverseIndexBuilder directly\n",
    "        if hasattr(tree, 'inverse_index_builder') and tree.inverse_index_builder:\n",
    "            builder = tree.inverse_index_builder\n",
    "            \n",
    "            # Calculate raw scores\n",
    "            raw_scores = builder.calculate_lexical_similarity(query)\n",
    "            \n",
    "            # Calculate normalized scores\n",
    "            normalized_scores = builder.calculate_normalized_lexical_similarity(query)\n",
    "            \n",
    "            # Get top 3 results for comparison\n",
    "            top_raw = sorted(raw_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            top_normalized = sorted(normalized_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            \n",
    "            print(\"\\nRAW SCORES (unbounded):\")\n",
    "            max_raw_score = max(raw_scores.values()) if raw_scores else 0\n",
    "            print(f\"  Max raw score: {max_raw_score:.3f}\")\n",
    "            for i, (node_id, score) in enumerate(top_raw, 1):\n",
    "                node = next((n for n in tree.tree_node_iterator() if n.node_id == node_id), None)\n",
    "                header = node.header if node else f\"Node {node_id}\"\n",
    "                print(f\"    {i}. [{node_id}] {header} (score: {score:.3f})\")\n",
    "            \n",
    "            print(\"\\nNORMALIZED SCORES (0-1.0 range):\")\n",
    "            max_normalized_score = max(normalized_scores.values()) if normalized_scores else 0\n",
    "            print(f\"  Max normalized score: {max_normalized_score:.3f}\")\n",
    "            for i, (node_id, score) in enumerate(top_normalized, 1):\n",
    "                node = next((n for n in tree.tree_node_iterator() if n.node_id == node_id), None)\n",
    "                header = node.header if node else f\"Node {node_id}\"\n",
    "                print(f\"    {i}. [{node_id}] {header} (score: {score:.3f})\")\n",
    "            \n",
    "            # Calculate maximum possible score for this query\n",
    "            query_tokens = builder._tokenize_and_clean(query)\n",
    "            query_monograms = query_tokens\n",
    "            query_bigrams = [f\"{query_tokens[i]} {query_tokens[i+1]}\" \n",
    "                            for i in range(len(query_tokens)-1)]\n",
    "            query_trigrams = [f\"{query_tokens[i]} {query_tokens[i+1]} {query_tokens[i+2]}\" \n",
    "                             for i in range(len(query_tokens)-2)]\n",
    "            \n",
    "            max_possible = builder._calculate_max_possible_score(query_monograms, query_bigrams, query_trigrams)\n",
    "            print(f\"\\nMax possible score for this query: {max_possible:.3f}\")\n",
    "            print(f\"Normalization factor: {max_raw_score/max_possible:.3f}\" if max_possible > 0 else \"N/A\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING ENHANCED SEARCH WITH NORMALIZED SCORES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test enhanced search which should now use normalized scores\n",
    "    test_query = \"chemistry atoms molecules\"\n",
    "    print(f\"\\nTesting enhanced_search with query: '{test_query}'\")\n",
    "    \n",
    "    enhanced_results = tree.enhanced_search(test_query, max_results=5)\n",
    "    print(f\"\\nEnhanced search results (using normalized lexical scores):\")\n",
    "    for i, (node_id, score) in enumerate(enhanced_results, 1):\n",
    "        node = next((n for n in tree.tree_node_iterator() if n.node_id == node_id), None)\n",
    "        header = node.header if node else f\"Node {node_id}\"\n",
    "        print(f\"  {i}. [{node_id}] {header} (combined score: {score:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING SEMANTIC + LEXICAL SCORE BALANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Demonstrate the score balance issue and solution\n",
    "    print(\"With normalized scores, semantic (0-1.0) and lexical (0-1.0) are now balanced!\")\n",
    "    print(\"Example weights: semantic_weight=0.6, lexical_weight=0.4\")\n",
    "    print(\"Max combined score would be: 0.6 * 1.0 + 0.4 * 1.0 = 1.0\")\n",
    "    \n",
    "    # Show some example calculations\n",
    "    semantic_weight = 0.6\n",
    "    lexical_weight = 0.4\n",
    "    \n",
    "    print(f\"\\nExample score combinations:\")\n",
    "    print(f\"  High semantic (0.9), High lexical (0.8): {semantic_weight * 0.9 + lexical_weight * 0.8:.3f}\")\n",
    "    print(f\"  Medium semantic (0.5), High lexical (0.9): {semantic_weight * 0.5 + lexical_weight * 0.9:.3f}\")\n",
    "    print(f\"  High semantic (0.8), Medium lexical (0.4): {semantic_weight * 0.8 + lexical_weight * 0.4:.3f}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = test_normalized_scoring()\n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ NORMALIZED SCORING TEST COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ùå SOME TESTS FAILED!\")\n",
    "        print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32492b30-847c-4a81-a677-adc1f446a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: torch in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from sentence-transformers) (0.33.1)\n",
      "Requirement already satisfied: Pillow in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: setuptools in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chemxai/miniforge3/envs/GenAI312/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.6.15)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Downloading scikit_learn-1.7.1-cp312-cp312-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m17.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:15\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sentence-transformers\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/4\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 scikit-learn-1.7.1 sentence-transformers-5.0.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install -U sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bbc00-3302-4490-ba6e-077cdf946034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
